---
title: "単回帰と重回帰：それぞれの使い分け"
description: "単回帰分析と重回帰分析の包括的な比較。モデル選択、多重共線性、調整済みR二乗、予測変数の追加がモデルを改善する場合と悪化させる場合について解説します。"
date: "2026-02-20"
category: "検定比較"
tags:
  - 単回帰
  - 重回帰
  - 線形回帰
  - 多重共線性
  - モデル選択
  - 決定係数
  - VIF
  - 予測変数
---

## はじめに

線形回帰は統計分析の中心的な手法です。最もシンプルな形である**単回帰分析**では、1つの予測変数を用いてアウトカム（結果変数）のばらつきを説明します。**重回帰分析**はこの枠組みを2つ以上の予測変数に拡張し、複雑な関係のモデリングや交絡変数の統制を可能にします。

しかし、予測変数が多ければ良いモデルになるとは限りません。無関係な変数の追加はモデルの複雑さを増し、多重共線性を引き起こし、新しいデータに対する予測精度を低下させる可能性があります。本記事では、それぞれのアプローチをいつ使うべきか、モデルの比較方法、そしてよくある落とし穴の回避方法を解説します。

勉強時間と試験成績の関係をモデリングする場合でも、住宅価格の包括的なモデルを構築する場合でも、シンプルさと複雑さのトレードオフを理解することが不可欠です。実際の分析は[単回帰計算機](/calculators/simple-regression)や[重回帰計算機](/calculators/multiple-regression)でお試しください。

## 比較早見表

| 特徴                   | 単回帰分析                       | 重回帰分析                               |
|------------------------|----------------------------------|------------------------------------------|
| 予測変数の数           | 1つ                              | 2つ以上                                  |
| 方程式                 | Y = b0 + b1*X                    | Y = b0 + b1*X1 + b2*X2 + ...            |
| 目的                   | 1つの関係を記述する              | 複数の要因でアウトカムをモデル化する     |
| 交絡変数の統制         | できない                         | できる（他の変数を一定に保持）           |
| 多重共線性のリスク     | 該当なし                         | あり                                     |
| モデル適合度の指標     | R二乗（決定係数）                | 調整済みR二乗                            |
| 過適合のリスク         | 低い                             | 予測変数が多いと高くなる                 |
| 解釈                   | 直感的                           | 注意が必要（偏回帰係数）                 |
| 可視化                 | 2次元散布図＋回帰直線            | 3次元を超えると可視化が困難              |

## 単回帰分析を使うべき場面

単回帰分析が適切なのは以下のような場合です。

1. **予測変数が1つだけの場合。** アウトカムとの関係を理解したい予測変数が1つだけある場合。

2. **探索的分析の段階。** より大きなモデルを構築する前に、単一の要因がアウトカムと関連しているかどうかを調べる場合。

3. **予測変数が唯一の関連変数である場合。** 他の要因が一定に保たれている統制実験では、1つの予測変数で十分な場合があります。

4. **コミュニケーションとシンプルさが優先される場合。** 予測変数が1つのモデルは、説明しやすく、可視化しやすく、専門外の聴衆にも伝えやすいです。

5. **サンプルサイズが小さい場合。** データが限られている場合、重回帰は過適合を起こしやすくなります。一般的な目安として、予測変数1つあたり少なくとも10〜20の観測値が必要です。

## 重回帰分析を使うべき場面

重回帰分析が適切なのは以下のような場合です。

1. **複数の要因がアウトカムに影響する場合。** 現実の多くの現象は複数の変数によって決まります。関連する予測変数を含めることで、理解と予測の両方が向上します。

2. **交絡変数を統制する必要がある場合。** 年齢と収入の両方が医療費に影響する場合、収入だけの単回帰では年齢による交絡が生じます。重回帰はそれぞれの効果を分離します。

3. **予測変数の相対的な重要性を比較したい場合。** 重回帰の標準化係数（ベータウェイト）は、どの予測変数が最も強い関連を持つかを明らかにします。

4. **予測モデルを構築する場合。** 関連する予測変数が多いほど（過適合しない限り）、予測精度は一般的に向上します。

5. **十分なサンプルサイズがある場合。** 予測変数1つあたり少なくとも10〜20の観測値があれば、安定した推定値が得られます。

## 例題データセット

不動産アナリストが住宅の販売価格を予測したいと考えています。データセットには20軒の住宅があり、3つの潜在的な予測変数があります：延べ床面積（平方フィート）、寝室数、最寄りの学校までの距離（マイル）。

| 住宅 | 延べ床面積 | 寝室数 | 距離（マイル） | 販売価格（千ドル） |
|------|------------|--------|----------------|---------------------|
| 1    | 1,200      | 2      | 0.8            | 215                 |
| 2    | 1,500      | 3      | 1.2            | 265                 |
| 3    | 1,800      | 3      | 0.5            | 310                 |
| 4    | 2,100      | 4      | 2.0            | 325                 |
| 5    | 1,350      | 2      | 1.5            | 230                 |
| 6    | 2,400      | 4      | 0.3            | 395                 |
| 7    | 1,600      | 3      | 1.0            | 280                 |
| 8    | 1,950      | 3      | 0.7            | 340                 |
| 9    | 1,100      | 2      | 2.5            | 190                 |
| 10   | 2,200      | 4      | 1.8            | 350                 |
| 11   | 1,750      | 3      | 0.6            | 305                 |
| 12   | 2,600      | 5      | 0.4            | 420                 |
| 13   | 1,450      | 2      | 1.3            | 250                 |
| 14   | 1,900      | 3      | 0.9            | 330                 |
| 15   | 2,300      | 4      | 1.1            | 370                 |
| 16   | 1,250      | 2      | 2.2            | 205                 |
| 17   | 2,050      | 4      | 1.6            | 345                 |
| 18   | 1,650      | 3      | 0.8            | 290                 |
| 19   | 2,500      | 5      | 0.5            | 410                 |
| 20   | 1,400      | 3      | 1.4            | 255                 |

### 記述統計量

| 変数             | 平均値  | 標準偏差 | 最小値 | 最大値 |
|------------------|---------|----------|--------|--------|
| 延べ床面積       | 1,780   | 438.7    | 1,100  | 2,600  |
| 寝室数           | 3.15    | 0.93     | 2      | 5      |
| 距離（マイル）   | 1.16    | 0.63     | 0.3    | 2.5    |
| 価格（千ドル）   | 304.0   | 63.5     | 190    | 420    |

## 単回帰分析：価格 vs 延べ床面積

### モデル

Price = b0 + b1 * SqFt

### 結果

| パラメータ | 推定値 | 標準誤差 | t値    | p値     |
|-----------|--------|----------|--------|---------|
| 切片       | 42.53  | 18.72    | 2.27   | 0.036   |
| 延べ床面積 | 0.147  | 0.010    | 14.27  | < 0.001 |

### モデル適合度

| 指標             | 値    |
|------------------|-------|
| R二乗            | 0.919 |
| 調整済みR二乗    | 0.914 |
| 残差標準誤差     | 18.61 |
| F統計量          | 203.6（自由度: 1, 18） |
| p値（モデル全体）| < 0.001 |

**解釈：** 延べ床面積が1平方フィート増加するごとに、予測販売価格は147ドル上昇します。延べ床面積だけで販売価格のばらつきの91.9%を説明しています。モデルは統計的に高度に有意です。

### 回帰式

Price（千ドル）= 42.53 + 0.147 * SqFt

2,000平方フィートの住宅の予測販売価格は：42.53 + 0.147 * 2000 = 336.5千ドルとなります。

## 重回帰分析：3つの予測変数すべてを使用

### モデル

Price = b0 + b1 * SqFt + b2 * Bedrooms + b3 * Distance

### 結果

| パラメータ | 推定値 | 標準誤差 | t値    | p値     |
|-----------|--------|----------|--------|---------|
| 切片       | 55.21  | 16.45    | 3.36   | 0.004   |
| 延べ床面積 | 0.112  | 0.015    | 7.47   | < 0.001 |
| 寝室数     | 15.83  | 7.92     | 2.00   | 0.063   |
| 距離       | -18.47 | 5.31     | -3.48  | 0.003   |

### モデル適合度

| 指標             | 値    |
|------------------|-------|
| R二乗            | 0.963 |
| 調整済みR二乗    | 0.956 |
| 残差標準誤差     | 13.35 |
| F統計量          | 138.9（自由度: 3, 16） |
| p値（モデル全体）| < 0.001 |

**解釈：**
- **延べ床面積（b1 = 0.112）：** 寝室数と距離を一定に保った場合、延べ床面積が1平方フィート増加するごとに価格は112ドル上昇します。これは単回帰モデル（0.147）よりも低い値です。延べ床面積に帰属していた効果の一部が寝室数と共有されているためです。
- **寝室数（b2 = 15.83, p = 0.063）：** 寝室が1室増えるごとに予測価格は15,830ドル上昇しますが、この効果は有意水準0.05では統計的に有意ではありません。
- **距離（b3 = -18.47, p = 0.003）：** 学校からの距離が1マイル増えるごとに価格は18,470ドル低下します。これは統計的に有意であり、実質的にも意味のある効果です。

## 2つのモデルの比較

| 指標               | 単回帰（延べ床面積のみ） | 重回帰（3予測変数） |
|--------------------|--------------------------|----------------------|
| R二乗              | 0.919                    | 0.963                |
| 調整済みR二乗      | 0.914                    | 0.956                |
| 残差標準誤差        | 18.61                    | 13.35                |
| AIC                | 176.4                    | 163.8                |
| BIC                | 179.4                    | 169.8                |
| 有意な予測変数      | 1/1                      | 2/3                  |

重回帰モデルは単回帰モデルよりも改善されています：
- 調整済みR二乗が0.914から0.956に増加。
- 残差標準誤差が18.61から13.35に減少（予測がより精密に）。
- AICとBICがともに減少（低いほど良い）。

寝室数は個別には有意ではありませんが（p = 0.063）、モデル全体の改善から3つの予測変数すべてを含める根拠があります。ただし、簡潔さを重視する分析者は寝室数を除外し、延べ床面積と距離のみを残すかもしれません。

### 縮小モデル：延べ床面積 + 距離

| パラメータ | 推定値 | 標準誤差 | t値    | p値     |
|-----------|--------|----------|--------|---------|
| 切片       | 72.18  | 14.23    | 5.07   | < 0.001 |
| 延べ床面積 | 0.131  | 0.009    | 14.56  | < 0.001 |
| 距離       | -20.91 | 5.12     | -4.08  | < 0.001 |

| 指標             | 値    |
|------------------|-------|
| R二乗            | 0.955 |
| 調整済みR二乗    | 0.950 |
| 残差標準誤差     | 14.22 |
| AIC              | 166.1 |
| BIC              | 170.1 |

この2変数モデルは優れた妥協案です。3変数モデルとほぼ同等の分散を説明しながら（調整済みR二乗 = 0.950 vs 0.956）、よりシンプルで、すべての予測変数が有意です。

## 多重共線性の理解

### 多重共線性とは？

多重共線性は、予測変数同士が高い相関を持つ場合に発生します。これは回帰の核心的な仮定には違反しませんが、以下の問題を引き起こします：

- 回帰係数の標準誤差が膨張する。
- モデル全体の適合度は良いにもかかわらず、個々の予測変数が非有意に見える。
- 係数が不安定になる（データの小さな変化が推定値の大きな変化につながる）。

### 相関行列

|              | 延べ床面積 | 寝室数 | 距離    |
|-------------|------------|--------|---------|
| 延べ床面積   | 1.000      | 0.912  | -0.287  |
| 寝室数       | 0.912      | 1.000  | -0.235  |
| 距離         | -0.287     | -0.235 | 1.000   |

延べ床面積と寝室数は高い相関を持っています（r = 0.912）。これが重回帰モデルで寝室数が有意にならなかった理由を説明しています。寝室数の情報は延べ床面積とほとんど重複しているのです。

### 分散膨張係数（VIF）

| 予測変数     | VIF   |
|-------------|-------|
| 延べ床面積   | 5.82  |
| 寝室数       | 5.41  |
| 距離         | 1.13  |

VIF値が5を超えると、延べ床面積と寝室数の間に中程度の多重共線性があることを示します。10を超えると深刻な問題があることを示します。今回のケースでは、延べ床面積と寝室数のVIFは懸念されるレベルですが、極端ではありません。

### 多重共線性の対処法

1. **相関の高い予測変数の一方を除外する。** 延べ床面積と寝室数が高い相関を持つ場合、理論的により関連が深い方、または予測力が強い方を残します。

2. **合成変数を作成する。** 相関の高い予測変数を組み合わせます（例：延べ床面積と寝室数から「住宅サイズ指数」を作成）。

3. **正則化を使用する。** リッジ回帰やLASSOは係数を縮小することで多重共線性に対処できます。

4. **変数を中心化または標準化する。** これだけでは多重共線性は解消しませんが、交互作用項の扱いに役立ちます。

## モデル選択の戦略

### 前進選択法

予測変数なしの状態から始めます。p値が最も低い変数を追加します（閾値以下の場合）。残りの予測変数がモデルを改善しなくなるまで追加を続けます。

### 後退消去法

すべての予測変数を含めた状態から始めます。p値が最も高い変数を除外します（閾値以上の場合）。残りのすべての予測変数が有意になるまで除外を続けます。

### ステップワイズ選択法

前進選択法と後退消去法を組み合わせます。各ステップでAICなどの基準に基づいて予測変数を追加または除外します。

### 情報量規準によるモデル比較

| モデル                        | AIC    | BIC    | 調整済みR二乗 |
|-------------------------------|--------|--------|----------------|
| 延べ床面積のみ                | 176.4  | 179.4  | 0.914          |
| 寝室数のみ                    | 180.2  | 183.2  | 0.892          |
| 距離のみ                      | 198.7  | 201.7  | 0.682          |
| 延べ床面積 + 距離             | 166.1  | 170.1  | 0.950          |
| 延べ床面積 + 寝室数           | 175.8  | 179.8  | 0.917          |
| 寝室数 + 距離                 | 172.4  | 176.4  | 0.928          |
| 延べ床面積 + 寝室数 + 距離    | 163.8  | 169.8  | 0.956          |

フルモデル（AIC = 163.8）と延べ床面積＋距離モデル（AIC = 166.1）が上位2つの候補です。AICの差は2.3と小さいです。多くの分析者は、よりシンプルな2変数モデルを選択するでしょう。

## 解釈における重要な違い

### 単回帰の係数

単回帰における延べ床面積の係数（0.147）は、延べ床面積と価格の間の**総合的な**関連を表しています。寝室数などの相関変数を通じた間接的な効果も含まれています。

### 重回帰の係数

重回帰における延べ床面積の係数（0.112）は、**偏回帰係数**（部分的な関連）を表しています。寝室数と距離を一定に保った場合の、延べ床面積が価格に与える効果です。個々の効果を分離したい場合、こちらの方がより意味のある値であることが多いです。

### 標準化係数（ベータウェイト）

重回帰における予測変数の相対的な重要性を比較するには、標準化係数を使用します。

| 予測変数     | 非標準化B | 標準化ベータ | 順位 |
|-------------|-----------|--------------|------|
| 延べ床面積   | 0.112     | 0.775        | 1    |
| 寝室数       | 15.83     | 0.232        | 3    |
| 距離         | -18.47    | -0.183       | 2    |

延べ床面積が最も大きな標準化効果を持ち（ベータ = 0.775）、次いで距離（ベータ = -0.183）、寝室数（ベータ = 0.232、ただし非有意）の順となっています。

## よくある落とし穴

1. **R二乗を上げるために予測変数を追加する。** R二乗は予測変数を追加すると必ず増加します（無関係な変数でも）。代わりに必ず**調整済みR二乗**や情報量規準（AIC/BIC）を使用してください。

2. **多重共線性を無視する。** 高いVIFは係数を信頼できなくします。個々の予測変数を解釈する前にVIFを確認してください。

3. **予測変数が多すぎて過適合する。** 20の観測値に10の予測変数がある場合、訓練データには良く適合しますが、汎化性能は低くなります。観測値と予測変数の比率は少なくとも10:1を維持してください。

4. **相関と因果関係を混同する。** 回帰は関連を特定するものであり、因果関係を示すものではありません。距離の係数は、学校の近くに引っ越すことで住宅価格が上がることを証明するものではありません。

5. **残差の仮定を確認しない。** 単回帰も重回帰も、線形性、独立性、等分散性、残差の正規性を仮定しています。残差と予測値のプロットで確認しましょう。

## 実際に試してみましょう

オンラインツールで回帰モデルを構築・比較してみましょう。

- [単回帰計算機](/calculators/simple-regression) — 単一予測変数のモデルに
- [重回帰計算機](/calculators/multiple-regression) — 複数予測変数のモデルに
- [相関分析計算機](/calculators/correlation) — まず変数間の関係を探索するために

## よくある質問（FAQ）

### 予測変数をいくつ含めるべきか？

**理論的根拠、調整済みR二乗、情報量規準（AIC/BIC）**を組み合わせて判断します。理論的に関連があり、モデル適合度を改善する予測変数を含めます。非有意でAIC/BICを改善しない予測変数は除外します。10:1ルール（予測変数1つあたり10の観測値）が実用的な上限の目安です。

### R二乗と調整済みR二乗の違いは？

**R二乗**は予測変数を追加すると必ず増加（または維持）します。有用かどうかに関わらずです。**調整済みR二乗**は予測変数の数にペナルティを課します。新しい予測変数がモデルを偶然以上に改善した場合にのみ増加します。異なる数の予測変数を持つモデルを比較する際は、必ず調整済みR二乗を報告してください。

### 重回帰で連続変数とカテゴリカル変数を混在させることはできますか？

はい。カテゴリカル変数は**ダミー変数**（指示変数とも呼ばれます）を使用して含めます。k個のカテゴリを持つカテゴリカル変数にはk-1個のダミー変数が必要です。例えば、「低」「中」「高」の3水準を持つ変数は2つのダミー変数で表現されます。

### 重回帰（multiple regression）と多変量回帰（multivariate regression）の違いは？

**重回帰（multiple regression）**はアウトカム変数が1つで予測変数が複数です。**多変量回帰（multivariate regression）**はアウトカム変数が複数です。これらの用語はしばしば混同されます。多くの研究者が「multivariate regression」と言う場合、実際には「multiple regression」を意味しています。

### 非線形関係はどのように確認しますか？

各予測変数をアウトカムに対して、また残差に対してプロットします。曲線的なパターンは非線形性を示唆します。対処法としては、多項式項（Xの二乗）の追加、対数変換の使用、一般化加法モデル（GAM）の適合などがあります。

### 重回帰を実行する前に予測変数を標準化すべきですか？

標準化（z得点への変換）は有効な推定のために必須ではありませんが、以下の場合に有用です：
- 予測変数の相対的な重要性を比較する場合（標準化ベータを用いて）。
- 予測変数のスケールが大きく異なる場合の数値的な問題を軽減する場合。
- 交互作用項の解釈を容易にする場合。

標準化してもしなくても、有意性検定とR二乗の値は同一です。

### 重回帰における抑制変数（サプレッサー変数）とは？

**抑制変数**とは、アウトカムとの直接的な相関は弱いにもかかわらず、モデルに含めることで他の変数の予測的妥当性を高める予測変数のことです。これにより、単回帰から重回帰に移行する際に係数の符号や大きさが変わることがあります。興味深い現象ですが、解釈を複雑にする可能性があるため、慎重に検討する必要があります。
